{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a906aeb6",
   "metadata": {},
   "source": [
    "The goal of this notebook is to automate the creation of an Obsidian Vault, containing the most well known ML and deep learning models for each datatype."
   ]
  },
  {
   "cell_type": "raw",
   "id": "810d627f",
   "metadata": {},
   "source": [
    "Next steps : \n",
    "    --> Try this method for other domains, ie: neuroscience, music...\n",
    "    --> Add metrics for each model (https://towardsdatascience.com/how-to-compare-machine-learning-algorithms-ccc266c4777)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f234c715",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9b4cfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT API Config\n",
    "from configparser import ConfigParser\n",
    "\n",
    "# Graph Viz\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import requests\n",
    "import openai\n",
    "import json\n",
    "import os\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea324d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API key stored in local cfg file\n",
    "# How to is available here : https://towardsdatascience.com/keeping-credentials-safe-in-jupyter-notebooks-fbd215a8e311\n",
    "\n",
    "parser = ConfigParser()\n",
    "_ = parser.read('ObsidianGPT.cfg')\n",
    "openai.api_key = parser.get('my_api', 'auth_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1ceb5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "davinci\n",
      "gpt-3.5-turbo-16k-0613\n",
      "text-davinci-001\n",
      "text-search-curie-query-001\n",
      "gpt-4\n",
      "babbage\n",
      "text-babbage-001\n",
      "gpt-3.5-turbo-16k\n",
      "curie-instruct-beta\n",
      "gpt-4-0314\n",
      "davinci-similarity\n",
      "code-davinci-edit-001\n",
      "text-similarity-curie-001\n",
      "gpt-4-0613\n",
      "ada-code-search-text\n",
      "text-search-ada-query-001\n",
      "babbage-search-query\n",
      "ada-similarity\n",
      "text-curie-001\n",
      "text-search-ada-doc-001\n",
      "text-search-babbage-query-001\n",
      "code-search-ada-code-001\n",
      "curie-search-document\n",
      "davinci-002\n",
      "gpt-3.5-turbo-0613\n",
      "text-search-davinci-query-001\n",
      "text-search-curie-doc-001\n",
      "babbage-search-document\n",
      "babbage-002\n",
      "babbage-code-search-text\n",
      "text-embedding-ada-002\n",
      "gpt-3.5-turbo\n",
      "davinci-instruct-beta\n",
      "davinci-search-query\n",
      "text-similarity-babbage-001\n",
      "text-davinci-002\n",
      "code-search-babbage-text-001\n",
      "text-search-davinci-doc-001\n",
      "code-search-ada-text-001\n",
      "ada-search-query\n",
      "text-similarity-ada-001\n",
      "ada-code-search-code\n",
      "whisper-1\n",
      "text-davinci-edit-001\n",
      "davinci-search-document\n",
      "curie-search-query\n",
      "babbage-similarity\n",
      "ada\n",
      "ada-search-document\n",
      "text-ada-001\n",
      "text-similarity-davinci-001\n",
      "curie-similarity\n",
      "babbage-code-search-code\n",
      "code-search-babbage-code-001\n",
      "text-search-babbage-doc-001\n",
      "gpt-3.5-turbo-0301\n",
      "curie\n",
      "text-davinci-003\n"
     ]
    }
   ],
   "source": [
    "#Shows available OPENAI models\n",
    "for i in openai.Model.list()[\"data\"]:\n",
    "    print(i[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fc36b2",
   "metadata": {},
   "source": [
    "# Functions definition"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc7f9ba7",
   "metadata": {},
   "source": [
    "# Future update : \n",
    "# Add try/except logic to generate_text function in order to retry after potential http bad responses \n",
    "\n",
    "try:\n",
    "    # Some Code\n",
    "except:\n",
    "    # Executed if error in the\n",
    "    # try block\n",
    "else:\n",
    "    # execute if no exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d576a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passes a specific prompt to the chosen OpenAI GPT model\n",
    "def generate_text(prompt, GPTmodel):\n",
    "    response = openai.ChatCompletion.create(model=GPTmodel,messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf59dbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spaces_to_camelcase(text_input):\n",
    "    text_output=re.sub(r\"(\\w)([A-Z])\", r\"\\1 \\2\", text_input)\n",
    "    return(text_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55b8f2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a local obsidian vault\n",
    "def create_obsidian_vault(vault_name, parent_directory):\n",
    "    '''Creates or Updates and Obsidian vault, in a given directory. Params are vault_name, parent_directory'''\n",
    "    vault_path = os.path.join(parent_directory, vault_name)\n",
    "    \n",
    "    # Create the vault directory\n",
    "    os.makedirs(vault_path, exist_ok=True)\n",
    "\n",
    "    # Create default folders within the vault\n",
    "    os.makedirs(os.path.join(vault_path, \"attachments\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(vault_path, \"notes\"), exist_ok=True)\n",
    "    \n",
    "    return vault_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbe2f2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract strings with regexp\n",
    "def regex_string_finder(input_string, pattern):\n",
    "    '''\n",
    "    Usage: regex_string_finder(\"This is a sample.text:with:colons\", r'\\.(.*?)\\:')\n",
    "    '''\n",
    "    # Use re.search to find the first match of the pattern\n",
    "    match = re.search(pattern, input_string)\n",
    "\n",
    "    # Check if a match is found\n",
    "    if match:\n",
    "        # Extract the substring between '.' and ':'\n",
    "        extracted_string = match.group(1)\n",
    "        return extracted_string.strip()\n",
    "    else:\n",
    "        return \"No match found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe3c070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_variable_to_text_file(variable, file_path):\n",
    "    # Open the file in write mode ('w')\n",
    "    with open(file_path, 'w') as file:\n",
    "        # Write the variable's value to the file\n",
    "        file.write(variable)\n",
    "    \n",
    "    print(f\"Variable written to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfb70b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_json_as_graph(data_dict):\n",
    "    # Create a directed graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Iterate through the JSON data and add nodes and edges to the graph\n",
    "    for category, subcategories in data_dict.items():\n",
    "        G.add_node(category, type=\"category\")\n",
    "        for subcategory, algorithms in subcategories.items():\n",
    "            G.add_node(subcategory, type=\"subcategory\")\n",
    "            G.add_edge(category, subcategory)\n",
    "            for algorithm in algorithms:\n",
    "                G.add_node(algorithm, type=\"algorithm\")\n",
    "                G.add_edge(subcategory, algorithm)\n",
    "\n",
    "    # Define node colors based on types\n",
    "    node_colors = {\n",
    "        \"category\": \"lightblue\",\n",
    "        \"subcategory\": \"lightgreen\",\n",
    "        \"algorithm\": \"lightcoral\",\n",
    "    }\n",
    "    for node in G.nodes():\n",
    "        if \"type\" not in G.nodes[node]:\n",
    "            G.nodes[node][\"type\"] = \"default\"\n",
    "\n",
    "    colors = [node_colors[G.nodes[node][\"type\"]] for node in G.nodes()]\n",
    "\n",
    "    # Create the graph visualization\n",
    "    pos = nx.spring_layout(G, seed=42)  # Position nodes using spring layout\n",
    "    nx.draw(G, pos, node_color=colors, with_labels=True, font_size=8)\n",
    "    plt.title(\"JSON Data Visualization as a Graph\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51859d3d",
   "metadata": {},
   "source": [
    "## GPT4 Base Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "045f43ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quite a bit of trial and error prompting led to the following prompt. Feel free to experiment with it!\n",
    "# Get the list of models for different kinds of data from the GPT API\n",
    "models_prompt = \"\"\"\n",
    "Provide an exhaustive list of popular machine learning and deep learning models for all types of data, \n",
    "grouped by data type and problem type.Provide as your response the dictionnary containing the results. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34eaa99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Get the list of models, for each data type and problem type\n",
    "models_text = generate_text(models_prompt, GPTmodel=\"gpt-4-0314\") #Points to the latest GPT4 model or gpt-3.5-turbo for faster results vs lower query quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4e4c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_string = output\n",
    "modified_string = re.sub(r'[ \\n]', '', original_string) #GPT is a conversational model and adds \\n and spaces, we remove them here to allow for dict to JSON conversion\n",
    "\n",
    "# Convert the string to a Python dictionary\n",
    "data_dict = json.loads(modified_string)\n",
    "\n",
    "# Now you can work with the data_dict as a Python dictionary\n",
    "print(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ba3bda",
   "metadata": {},
   "source": [
    "## Save base JSON to disk, as txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cc810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save models dict to disk with current date\n",
    "with open(f'models_v1_3_{today.date()}.txt', 'w') as convert_file:\n",
    "     convert_file.write(json.dumps(data_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b7dbf9",
   "metadata": {},
   "source": [
    "## Let's visualise what we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03e25e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updates since v1 : \n",
    "# On the 2023-09-06 : Linkedin, Medium, TowardsDataScience links are almost all broken. It's not worth asking GPT4\n",
    "# On the 2023-09-06 : Model completion has been switched to gpt3.5 turbo since cheaper and faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cf8ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_json_as_graph(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688b5069",
   "metadata": {},
   "source": [
    "Not the clearest viz, let's try an interactive one in a separate Notebook using Dash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5769b74a",
   "metadata": {},
   "source": [
    "## Let's add details for each of our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a932d28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file from disk if necessary\n",
    "filename = f'models_v1_3_2023-09-07.txt'\n",
    "with open(filename, 'r') as read_file:\n",
    "    data_dict = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "58cf069d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['StructuredData', 'TimeSeriesData', 'TextData', 'ImageData', 'AudioData', 'GraphData'])\n"
     ]
    }
   ],
   "source": [
    "print(data_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ba1c2cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** DATA TYPE : StructuredData ***\n",
      "*** SUBCATEGORY : Regression ***\n",
      "*** ALGORITHM : LinearRegression ***\n",
      "*** ALGORITHM : RidgeRegression ***\n",
      "*** ALGORITHM : LassoRegression ***\n",
      "*** ALGORITHM : ElasticNet ***\n",
      "*** ALGORITHM : SupportVectorRegression ***\n",
      "*** ALGORITHM : DecisionTreeRegression ***\n",
      "*** ALGORITHM : RandomForestRegression ***\n",
      "*** ALGORITHM : AdaBoostRegression ***\n",
      "*** ALGORITHM : GradientBoostingRegression ***\n",
      "*** ALGORITHM : XGBoost ***\n",
      "*** ALGORITHM : LightGBM ***\n",
      "*** ALGORITHM : CatBoost ***\n",
      "*** ALGORITHM : ArtificialNeuralNetworks ***\n",
      "*** ALGORITHM : LongShort-TermMemory ***\n",
      "*** SUBCATEGORY : Classification ***\n",
      "*** ALGORITHM : LogisticRegression ***\n",
      "*** ALGORITHM : LinearDiscriminantAnalysis ***\n",
      "*** ALGORITHM : QuadraticDiscriminantAnalysis ***\n",
      "*** ALGORITHM : SupportVectorMachines ***\n",
      "*** ALGORITHM : DecisionTreeClassifier ***\n",
      "*** ALGORITHM : RandomForestClassifier ***\n",
      "*** ALGORITHM : AdaBoostClassifier ***\n",
      "*** ALGORITHM : GradientBoostingClassifier ***\n",
      "*** ALGORITHM : XGBoostClassifier ***\n",
      "*** ALGORITHM : LightGBMClassifier ***\n",
      "*** ALGORITHM : CatBoostClassifier ***\n",
      "*** ALGORITHM : K-NearestNeighbors ***\n",
      "*** ALGORITHM : NaiveBayesClassifier ***\n",
      "*** ALGORITHM : ArtificialNeuralNetworks ***\n",
      "*** SUBCATEGORY : Clustering ***\n",
      "*** ALGORITHM : K-MeansClustering ***\n",
      "*** ALGORITHM : DBSCAN ***\n",
      "*** ALGORITHM : HierarchicalClustering ***\n",
      "*** ALGORITHM : SpectralClustering ***\n",
      "*** ALGORITHM : MeanShift ***\n",
      "*** ALGORITHM : AffinityPropagation ***\n",
      "*** ALGORITHM : OPTICS ***\n",
      "*** ALGORITHM : BIRCH ***\n",
      "*** SUBCATEGORY : DimensionalityReduction ***\n",
      "*** ALGORITHM : PrincipalComponentAnalysis ***\n",
      "*** ALGORITHM : LinearDiscriminantAnalysis ***\n",
      "*** ALGORITHM : t-DistributedStochasticNeighborEmbedding ***\n",
      "*** ALGORITHM : FactorAnalysis ***\n",
      "*** ALGORITHM : IndependentComponentAnalysis ***\n",
      "*** ALGORITHM : UniformManifoldApproximationandProjection ***\n",
      "*** ALGORITHM : Autoencoders ***\n",
      "*** SUBCATEGORY : AnomalyDetection ***\n",
      "*** ALGORITHM : IsolationForest ***\n",
      "*** ALGORITHM : LOF ***\n",
      "*** ALGORITHM : One-ClassSVM ***\n",
      "*** ALGORITHM : EllipticEnvelope ***\n",
      "*** ALGORITHM : DBSCAN ***\n",
      "*** ALGORITHM : K-means ***\n",
      "*** ALGORITHM : Autoencoders ***\n",
      "*** SUBCATEGORY : ReinforcementLearning ***\n",
      "*** ALGORITHM : Q-Learning ***\n",
      "*** ALGORITHM : SARSA ***\n",
      "*** ALGORITHM : DeepQ-Network ***\n",
      "*** ALGORITHM : PolicyGradientMethods ***\n",
      "*** ALGORITHM : REINFORCE ***\n",
      "*** ALGORITHM : ProximalPolicyOptimization ***\n",
      "*** ALGORITHM : SoftActor-Critic ***\n",
      "*** ALGORITHM : MonteCarloTreeSearch ***\n",
      "*** DATA TYPE : TimeSeriesData ***\n",
      "*** SUBCATEGORY : Forecasting ***\n",
      "*** ALGORITHM : AutoregressiveIntegratedMovingAverage ***\n",
      "*** ALGORITHM : ExponentialSmoothingStateSpaceModel ***\n",
      "*** ALGORITHM : SeasonalDecompositionofTimeSeries ***\n",
      "*** ALGORITHM : Prophet ***\n",
      "*** ALGORITHM : ThetaMethods ***\n",
      "*** ALGORITHM : RecurrentNeuralNetworks ***\n",
      "*** ALGORITHM : LongShort-TermMemory ***\n",
      "*** ALGORITHM : GatedRecurrentUnits ***\n",
      "*** ALGORITHM : ConvolutionalNeuralNetworks ***\n",
      "*** ALGORITHM : WaveNet ***\n",
      "*** SUBCATEGORY : AnomalyDetection ***\n",
      "*** ALGORITHM : Seasonal-TrendDecompositionusingLoess ***\n",
      "*** ALGORITHM : BayesianStructuralTimeSeriesModels ***\n",
      "*** ALGORITHM : AutoregressiveIntegratedMovingAverage ***\n",
      "*** ALGORITHM : LongShort-TermMemory ***\n",
      "*** ALGORITHM : GatedRecurrentUnits ***\n",
      "*** ALGORITHM : Autoencoders ***\n",
      "*** DATA TYPE : TextData ***\n",
      "*** SUBCATEGORY : NaturalLanguageProcessing ***\n",
      "*** ALGORITHM : Bag-of-Words ***\n",
      "*** ALGORITHM : TF-IDF ***\n",
      "*** ALGORITHM : Word2Vec ***\n",
      "*** ALGORITHM : GloVe ***\n",
      "*** ALGORITHM : FastText ***\n",
      "*** ALGORITHM : ElMo ***\n",
      "*** ALGORITHM : BERT ***\n",
      "*** ALGORITHM : GPT ***\n",
      "*** ALGORITHM : GPT-2 ***\n",
      "*** ALGORITHM : GPT-3 ***\n",
      "*** ALGORITHM : RoBERTa ***\n",
      "*** ALGORITHM : XLNet ***\n",
      "*** ALGORITHM : ALBERT ***\n",
      "*** ALGORITHM : T5 ***\n",
      "*** ALGORITHM : DistilBERT ***\n",
      "*** ALGORITHM : MobileBERT ***\n",
      "*** ALGORITHM : UniLM ***\n",
      "*** ALGORITHM : OpenAICodex ***\n",
      "*** ALGORITHM : RecurrentNeuralNetworks ***\n",
      "*** ALGORITHM : LongShort-TermMemory ***\n",
      "*** ALGORITHM : GatedRecurrentUnits ***\n",
      "*** ALGORITHM : ConvolutionalNeuralNetworks ***\n",
      "*** ALGORITHM : TransformerModels ***\n",
      "*** ALGORITHM : AttentionMechanisms ***\n",
      "*** ALGORITHM : Sequence-to-SequenceModels ***\n",
      "*** SUBCATEGORY : SentimentAnalysis ***\n",
      "*** ALGORITHM : NaiveBayes ***\n",
      "*** ALGORITHM : LogisticRegression ***\n",
      "*** ALGORITHM : SupportVectorMachines ***\n",
      "*** ALGORITHM : RandomForest ***\n",
      "*** ALGORITHM : XGBoost ***\n",
      "*** ALGORITHM : RecurrentNeuralNetworks ***\n",
      "*** ALGORITHM : LongShort-TermMemory ***\n",
      "*** ALGORITHM : GatedRecurrentUnits ***\n",
      "*** ALGORITHM : ConvolutionalNeuralNetworks ***\n",
      "*** ALGORITHM : BERT ***\n",
      "*** ALGORITHM : GPT ***\n",
      "*** SUBCATEGORY : TopicModeling ***\n",
      "*** ALGORITHM : LatentDirichletAllocation ***\n",
      "*** ALGORITHM : Non-negativeMatrixFactorization ***\n",
      "*** ALGORITHM : LatentSemanticAnalysis ***\n",
      "*** ALGORITHM : CorrelationalCorrespondenceAnalysis ***\n",
      "*** DATA TYPE : ImageData ***\n",
      "*** SUBCATEGORY : ImageClassification ***\n",
      "*** ALGORITHM : ConvolutionalNeuralNetworks ***\n",
      "*** ALGORITHM : ResNet ***\n",
      "*** ALGORITHM : Inception ***\n",
      "*** ALGORITHM : VGG ***\n",
      "*** ALGORITHM : MobileNet ***\n",
      "*** ALGORITHM : EfficientNet ***\n",
      "*** ALGORITHM : DenseNet ***\n",
      "*** ALGORITHM : Xception ***\n",
      "*** ALGORITHM : NASNet ***\n",
      "*** SUBCATEGORY : ImageSegmentation ***\n",
      "*** ALGORITHM : FullyConvolutionalNetworks ***\n",
      "*** ALGORITHM : U-Net ***\n",
      "*** ALGORITHM : MaskR-CNN ***\n",
      "*** ALGORITHM : DeepLab ***\n",
      "*** ALGORITHM : PSPNet ***\n",
      "*** ALGORITHM : HRNet ***\n",
      "*** SUBCATEGORY : ObjectDetection ***\n",
      "*** ALGORITHM : R-CNN ***\n",
      "*** ALGORITHM : FastR-CNN ***\n",
      "*** ALGORITHM : FasterR-CNN ***\n",
      "*** ALGORITHM : YOLO ***\n",
      "*** ALGORITHM : SSD ***\n",
      "*** ALGORITHM : RetinaNet ***\n",
      "*** ALGORITHM : CenterNet ***\n",
      "*** ALGORITHM : EfficientDet ***\n",
      "*** SUBCATEGORY : ImageCaptioning ***\n",
      "*** ALGORITHM : Encoder-DecoderArchitectures ***\n",
      "*** ALGORITHM : AttentionModels ***\n",
      "*** ALGORITHM : ShowandTell ***\n",
      "*** ALGORITHM : Show,AttendandTell ***\n",
      "*** ALGORITHM : AdaptiveAttention ***\n",
      "*** ALGORITHM : TransformerModels ***\n",
      "*** SUBCATEGORY : StyleTransfer ***\n",
      "*** ALGORITHM : NeuralStyleTransfer ***\n",
      "*** ALGORITHM : CycleGAN ***\n",
      "*** ALGORITHM : StarGAN ***\n",
      "*** ALGORITHM : AdaIN ***\n",
      "*** SUBCATEGORY : GenerativeModels ***\n",
      "*** ALGORITHM : Autoencoders ***\n",
      "*** ALGORITHM : VariationalAutoencoders ***\n",
      "*** ALGORITHM : GenerativeAdversarialNetworks ***\n",
      "*** ALGORITHM : DCGAN ***\n",
      "*** ALGORITHM : WassersteinGAN ***\n",
      "*** ALGORITHM : CycleGAN ***\n",
      "*** ALGORITHM : StyleGAN ***\n",
      "*** ALGORITHM : BigGAN ***\n",
      "*** DATA TYPE : AudioData ***\n",
      "*** SUBCATEGORY : SpeechRecognition ***\n",
      "*** ALGORITHM : HiddenMarkovModels ***\n",
      "*** ALGORITHM : GaussianMixtureModels ***\n",
      "*** ALGORITHM : DeepNeuralNetworks ***\n",
      "*** ALGORITHM : ConnectionistTemporalClassification ***\n",
      "*** ALGORITHM : RecurrentNeuralNetworks ***\n",
      "*** ALGORITHM : LongShort-TermMemory ***\n",
      "*** ALGORITHM : GatedRecurrentUnits ***\n",
      "*** ALGORITHM : Listen,Attend,andSpell ***\n",
      "*** ALGORITHM : WaveNet ***\n",
      "*** ALGORITHM : Tacotron ***\n",
      "*** ALGORITHM : TransformerModels ***\n",
      "*** SUBCATEGORY : SpeakerIdentification/Verification ***\n",
      "*** ALGORITHM : GaussianMixtureModels ***\n",
      "*** ALGORITHM : SupportVectorMachines ***\n",
      "*** ALGORITHM : i-vectors ***\n",
      "*** ALGORITHM : DeepSpeakerEmbeddings ***\n",
      "*** ALGORITHM : RecurrentNeuralNetworks ***\n",
      "*** ALGORITHM : LongShort-TermMemory ***\n",
      "*** ALGORITHM : GatedRecurrentUnits ***\n",
      "*** SUBCATEGORY : AudioClassification ***\n",
      "*** ALGORITHM : MelSpectrogram ***\n",
      "*** ALGORITHM : MFCC ***\n",
      "*** ALGORITHM : ConvolutionalNeuralNetworks ***\n",
      "*** ALGORITHM : RecurrentNeuralNetworks ***\n",
      "*** ALGORITHM : LongShort-TermMemory ***\n",
      "*** ALGORITHM : GatedRecurrentUnits ***\n",
      "*** ALGORITHM : AttentionModels ***\n",
      "*** ALGORITHM : WaveNet ***\n",
      "*** ALGORITHM : Spectrogram ***\n",
      "*** SUBCATEGORY : MusicGeneration ***\n",
      "*** ALGORITHM : MarkovModels ***\n",
      "*** ALGORITHM : RestrictedBoltzmannMachines ***\n",
      "*** ALGORITHM : RecurrentNeuralNetworks ***\n",
      "*** ALGORITHM : LongShort-TermMemory ***\n",
      "*** ALGORITHM : GatedRecurrentUnits ***\n",
      "*** ALGORITHM : WaveNet ***\n",
      "*** ALGORITHM : TransformerModels ***\n",
      "*** SUBCATEGORY : MusicRecommendation ***\n",
      "*** ALGORITHM : CollaborativeFiltering ***\n",
      "*** ALGORITHM : MatrixFactorization ***\n",
      "*** ALGORITHM : Autoencoders ***\n",
      "*** ALGORITHM : RestrictedBoltzmannMachines ***\n",
      "*** ALGORITHM : DeepNeuralNetworks ***\n",
      "*** ALGORITHM : RecurrentNeuralNetworks ***\n",
      "*** ALGORITHM : LongShort-TermMemory ***\n",
      "*** ALGORITHM : GatedRecurrentUnits ***\n",
      "*** DATA TYPE : GraphData ***\n",
      "*** SUBCATEGORY : GraphNetworks ***\n",
      "*** ALGORITHM : GraphConvolutionalNetworks ***\n",
      "*** ALGORITHM : GraphSAGE ***\n",
      "*** ALGORITHM : GraphAttentionNetworks ***\n",
      "*** ALGORITHM : GraphIsomorphismNetworks ***\n",
      "*** ALGORITHM : ChebNet ***\n",
      "*** ALGORITHM : GINConv ***\n",
      "*** ALGORITHM : GraphNeuralNetworks ***\n",
      "*** ALGORITHM : DynamicGraphRepresentationLearning ***\n",
      "*** SUBCATEGORY : GraphClustering ***\n",
      "*** ALGORITHM : SpectralClustering ***\n",
      "*** ALGORITHM : GraphModularityMaximization ***\n",
      "*** ALGORITHM : LouvainAlgorithm ***\n",
      "*** ALGORITHM : Infomap ***\n",
      "*** ALGORITHM : LabelPropagationAlgorithm ***\n",
      "*** SUBCATEGORY : GraphEmbeddings ***\n",
      "*** ALGORITHM : DeepWalk ***\n",
      "*** ALGORITHM : Node2Vec ***\n",
      "*** ALGORITHM : GraphConvolutionalNetwork ***\n",
      "*** ALGORITHM : StructuralDeepNetworkEmbeddings ***\n",
      "*** ALGORITHM : GE-SNMF ***\n"
     ]
    }
   ],
   "source": [
    "# Check if loop structure is valid\n",
    "for data_type in data_dict.keys():\n",
    "    print(f\"*** DATA TYPE : {data_type} ***\")\n",
    "    for subcategory in data_dict[data_type]:\n",
    "        print(f\"*** SUBCATEGORY : {subcategory} ***\")\n",
    "        for algorithm in data_dict[data_type][subcategory]:\n",
    "            print(f\"*** ALGORITHM : {algorithm} ***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ad958c",
   "metadata": {},
   "source": [
    "# Prompt GPT and write to Obsidian Vault"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f891ee68",
   "metadata": {},
   "source": [
    "# A bit of history \n",
    "\n",
    "# On the 2023-05-05, the cell below cost 4$ in GPT4 API calls, and took 4 hours to execute\n",
    "\n",
    "# On the 2023-09-05,ITS BROKEN. So the cell below cost 0.51$ in GPT4 API calls, and took 19 minutes to execute\n",
    "\n",
    "# On the 2023-09-06, ITS BROKEN, does not create an entry for each model. It bunches them up in one entry under model type. The cell below cost 0.82$ in GPT4 API calls, and took 24 minutes to execute\n",
    "\n",
    "# On the 2023-09-12, it's modified and works! Switch to GPT3.5 for model details completion, for costs reasons. Also removed python snippet because more up-to-date methods available in through best implementation links\n",
    "#   The cell below cost 0.31$ in GPT3.5turbo API calls, and took 63 minutes to execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5747326b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/WDescamps/Desktop/code_projects/side_projects/ObsidianGPT/Vault_v1_3-2023-09-12'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Obsidian Vault\n",
    "\n",
    "today=datetime.datetime.now()\n",
    "vault_path = create_obsidian_vault(vault_name=f\"Vault_v1_3\"+ \"-\" + str(today.date()), parent_directory=os.getcwd())\n",
    "vault_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bf2eb201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "outer:   0%|                                                             | 0/6 [00:00<?, ?it/s]\n",
      "inner:   0%|                                                             | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "inner:  11%|█████▉                                               | 1/9 [00:44<05:56, 44.52s/it]\u001b[A\n",
      "inner:  22%|███████████▊                                         | 2/9 [01:30<05:19, 45.59s/it]\u001b[A\n",
      "inner:  33%|█████████████████▋                                   | 3/9 [02:00<03:50, 38.44s/it]\u001b[A\n",
      "inner:  44%|███████████████████████▌                             | 4/9 [02:44<03:23, 40.61s/it]\u001b[A\n",
      "inner:  56%|█████████████████████████████▍                       | 5/9 [03:21<02:36, 39.13s/it]\u001b[A\n",
      "inner:  67%|███████████████████████████████████▎                 | 6/9 [03:56<01:53, 37.67s/it]\u001b[A\n",
      "inner:  78%|█████████████████████████████████████████▏           | 7/9 [04:30<01:13, 36.74s/it]\u001b[A\n",
      "inner:  89%|███████████████████████████████████████████████      | 8/9 [05:08<00:37, 37.01s/it]\u001b[A\n",
      "inner: 100%|█████████████████████████████████████████████████████| 9/9 [05:46<00:00, 38.50s/it]\u001b[A\n",
      "outer:  17%|████████▋                                           | 1/6 [05:46<28:52, 346.49s/it]\n",
      "inner:   0%|                                                             | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "inner:  17%|████████▊                                            | 1/6 [00:29<02:27, 29.51s/it]\u001b[A\n",
      "inner:  33%|█████████████████▋                                   | 2/6 [01:05<02:12, 33.10s/it]\u001b[A\n",
      "inner:  50%|██████████████████████████▌                          | 3/6 [01:35<01:35, 31.73s/it]\u001b[A\n",
      "inner:  67%|███████████████████████████████████▎                 | 4/6 [02:08<01:04, 32.45s/it]\u001b[A\n",
      "inner:  83%|████████████████████████████████████████████▏        | 5/6 [02:46<00:34, 34.25s/it]\u001b[A\n",
      "inner: 100%|█████████████████████████████████████████████████████| 6/6 [03:16<00:00, 32.71s/it]\u001b[A\n",
      "outer:  33%|█████████████████▎                                  | 2/6 [09:02<17:12, 258.13s/it]\n",
      "inner:   0%|                                                             | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "inner:  12%|██████▋                                              | 1/8 [00:33<03:53, 33.34s/it]\u001b[A\n",
      "inner:  25%|█████████████▎                                       | 2/8 [01:06<03:20, 33.40s/it]\u001b[A\n",
      "inner:  38%|███████████████████▉                                 | 3/8 [01:41<02:51, 34.23s/it]\u001b[A\n",
      "inner:  50%|██████████████████████████▌                          | 4/8 [02:10<02:08, 32.03s/it]\u001b[A\n",
      "inner:  62%|█████████████████████████████████▏                   | 5/8 [02:39<01:32, 30.79s/it]\u001b[A\n",
      "inner:  75%|███████████████████████████████████████▊             | 6/8 [03:09<01:01, 30.58s/it]\u001b[A\n",
      "inner:  88%|██████████████████████████████████████████████▍      | 7/8 [03:38<00:30, 30.20s/it]\u001b[A\n",
      "inner: 100%|█████████████████████████████████████████████████████| 8/8 [04:06<00:00, 30.81s/it]\u001b[A\n",
      "outer:  50%|██████████████████████████                          | 3/6 [13:09<12:38, 252.83s/it]\n",
      "inner:   0%|                                                             | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "inner:  17%|████████▊                                            | 1/6 [00:41<03:27, 41.50s/it]\u001b[A\n",
      "inner:  33%|█████████████████▋                                   | 2/6 [01:11<02:18, 34.68s/it]\u001b[A\n",
      "inner:  50%|██████████████████████████▌                          | 3/6 [02:00<02:03, 41.11s/it]\u001b[A\n",
      "inner:  67%|███████████████████████████████████▎                 | 4/6 [02:32<01:15, 37.65s/it]\u001b[A\n",
      "inner:  83%|████████████████████████████████████████████▏        | 5/6 [03:10<00:37, 37.93s/it]\u001b[A\n",
      "inner: 100%|█████████████████████████████████████████████████████| 6/6 [03:47<00:00, 37.88s/it]\u001b[A\n",
      "outer:  67%|██████████████████████████████████▋                 | 4/6 [16:56<08:05, 242.74s/it]\n",
      "inner:   0%|                                                             | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "inner:  25%|█████████████▎                                       | 1/4 [00:29<01:28, 29.38s/it]\u001b[A\n",
      "inner:  50%|██████████████████████████▌                          | 2/4 [01:07<01:09, 34.72s/it]\u001b[A\n",
      "inner:  75%|███████████████████████████████████████▊             | 3/4 [01:48<00:37, 37.25s/it]\u001b[A\n",
      "inner: 100%|█████████████████████████████████████████████████████| 4/4 [02:18<00:00, 34.71s/it]\u001b[A\n",
      "outer:  83%|███████████████████████████████████████████▎        | 5/6 [19:15<03:25, 205.27s/it]\n",
      "inner:   0%|                                                             | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "inner:  12%|██████▋                                              | 1/8 [00:35<04:09, 35.58s/it]\u001b[A\n",
      "inner:  25%|█████████████▎                                       | 2/8 [01:16<03:51, 38.64s/it]\u001b[A\n",
      "inner:  38%|███████████████████▉                                 | 3/8 [01:58<03:22, 40.43s/it]\u001b[A\n",
      "inner:  50%|██████████████████████████▌                          | 4/8 [02:38<02:39, 39.91s/it]\u001b[A\n",
      "inner:  62%|█████████████████████████████████▏                   | 5/8 [03:16<01:57, 39.26s/it]\u001b[A\n",
      "inner:  75%|███████████████████████████████████████▊             | 6/8 [03:58<01:20, 40.15s/it]\u001b[A\n",
      "inner:  88%|██████████████████████████████████████████████▍      | 7/8 [04:49<00:43, 43.86s/it]\u001b[A\n",
      "inner: 100%|█████████████████████████████████████████████████████| 8/8 [05:19<00:00, 39.93s/it]\u001b[A\n",
      "outer: 100%|████████████████████████████████████████████████████| 6/6 [24:34<00:00, 245.81s/it]\n",
      "outer:   0%|                                                             | 0/5 [00:00<?, ?it/s]\n",
      "inner:   0%|                                                            | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "inner:   9%|████▋                                               | 1/11 [00:30<05:05, 30.58s/it]\u001b[A\n",
      "inner:  18%|█████████▍                                          | 2/11 [01:17<06:02, 40.27s/it]\u001b[A\n",
      "inner:  27%|██████████████▏                                     | 3/11 [01:56<05:15, 39.44s/it]\u001b[A\n",
      "inner:  36%|██████████████████▉                                 | 4/11 [02:33<04:30, 38.69s/it]\u001b[A\n",
      "inner:  45%|███████████████████████▋                            | 5/11 [03:15<03:58, 39.68s/it]\u001b[A\n",
      "inner:  55%|████████████████████████████▎                       | 6/11 [03:57<03:23, 40.63s/it]\u001b[A\n",
      "inner:  64%|█████████████████████████████████                   | 7/11 [04:35<02:38, 39.68s/it]\u001b[A\n",
      "inner:  73%|█████████████████████████████████████▊              | 8/11 [05:10<01:54, 38.11s/it]\u001b[A\n",
      "inner:  82%|██████████████████████████████████████████▌         | 9/11 [05:42<01:12, 36.28s/it]\u001b[A\n",
      "inner:  91%|██████████████████████████████████████████████▎    | 10/11 [06:14<00:35, 35.03s/it]\u001b[A\n",
      "inner: 100%|███████████████████████████████████████████████████| 11/11 [06:54<00:00, 37.70s/it]\u001b[A\n",
      "outer:  20%|██████████▍                                         | 1/5 [06:54<27:38, 414.72s/it]\n",
      "inner:   0%|                                                             | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "inner:  14%|███████▌                                             | 1/7 [00:36<03:41, 36.84s/it]\u001b[A\n",
      "inner:  29%|███████████████▏                                     | 2/7 [01:09<02:51, 34.34s/it]\u001b[A\n",
      "inner:  43%|██████████████████████▋                              | 3/7 [01:42<02:15, 33.94s/it]\u001b[A\n",
      "inner:  57%|██████████████████████████████▎                      | 4/7 [02:21<01:47, 35.86s/it]\u001b[A\n",
      "inner:  71%|█████████████████████████████████████▊               | 5/7 [02:49<01:06, 33.13s/it]\u001b[A\n",
      "inner:  86%|█████████████████████████████████████████████▍       | 6/7 [03:21<00:32, 32.71s/it]\u001b[A\n",
      "inner: 100%|█████████████████████████████████████████████████████| 7/7 [04:09<00:00, 35.66s/it]\u001b[A\n",
      "outer:  40%|████████████████████▊                               | 2/5 [11:04<15:52, 317.63s/it]\n",
      "inner:   0%|                                                             | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "inner:  11%|█████▉                                               | 1/9 [00:26<03:31, 26.45s/it]\u001b[A\n",
      "inner:  22%|███████████▊                                         | 2/9 [01:00<03:35, 30.83s/it]\u001b[A\n",
      "inner:  33%|█████████████████▋                                   | 3/9 [01:36<03:20, 33.44s/it]\u001b[A\n",
      "inner:  44%|███████████████████████▌                             | 4/9 [02:11<02:48, 33.79s/it]\u001b[A\n",
      "inner:  56%|█████████████████████████████▍                       | 5/9 [02:40<02:08, 32.10s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inner:  67%|███████████████████████████████████▎                 | 6/9 [03:19<01:43, 34.48s/it]\u001b[A\n",
      "inner:  78%|█████████████████████████████████████████▏           | 7/9 [03:50<01:06, 33.38s/it]\u001b[A\n",
      "inner:  89%|███████████████████████████████████████████████      | 8/9 [04:24<00:33, 33.48s/it]\u001b[A\n",
      "inner: 100%|█████████████████████████████████████████████████████| 9/9 [04:55<00:00, 32.83s/it]\u001b[A\n",
      "outer:  60%|███████████████████████████████▏                    | 3/5 [15:59<10:15, 307.51s/it]\n",
      "inner:   0%|                                                             | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "inner:  14%|███████▌                                             | 1/7 [00:26<02:38, 26.42s/it]\u001b[A\n",
      "inner:  29%|███████████████▏                                     | 2/7 [01:04<02:44, 33.00s/it]\u001b[A\n",
      "inner:  43%|██████████████████████▋                              | 3/7 [01:46<02:28, 37.14s/it]\u001b[A\n",
      "inner:  57%|██████████████████████████████▎                      | 4/7 [02:12<01:39, 33.04s/it]\u001b[A\n",
      "inner:  71%|█████████████████████████████████████▊               | 5/7 [02:45<01:05, 32.97s/it]\u001b[A\n",
      "inner:  86%|█████████████████████████████████████████████▍       | 6/7 [03:15<00:31, 31.93s/it]\u001b[A\n",
      "inner: 100%|█████████████████████████████████████████████████████| 7/7 [03:52<00:00, 33.16s/it]\u001b[A\n",
      "outer:  80%|█████████████████████████████████████████▌          | 4/5 [19:51<04:37, 277.74s/it]\n",
      "inner:   0%|                                                             | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "inner:  12%|██████▋                                              | 1/8 [00:35<04:10, 35.85s/it]\u001b[A\n",
      "inner:  25%|█████████████▎                                       | 2/8 [01:01<02:58, 29.78s/it]\u001b[A\n",
      "inner:  38%|███████████████████▉                                 | 3/8 [01:33<02:35, 31.05s/it]\u001b[A\n",
      "inner:  50%|██████████████████████████▌                          | 4/8 [02:11<02:14, 33.68s/it]\u001b[A\n",
      "inner:  62%|█████████████████████████████████▏                   | 5/8 [02:46<01:42, 34.12s/it]\u001b[A\n",
      "inner:  75%|███████████████████████████████████████▊             | 6/8 [03:29<01:14, 37.12s/it]\u001b[A\n",
      "inner:  88%|██████████████████████████████████████████████▍      | 7/8 [04:07<00:37, 37.57s/it]\u001b[A\n",
      "inner: 100%|█████████████████████████████████████████████████████| 8/8 [04:40<00:00, 35.03s/it]\u001b[A\n",
      "outer: 100%|████████████████████████████████████████████████████| 5/5 [24:32<00:00, 294.44s/it]\n",
      "outer:   0%|                                                             | 0/3 [00:00<?, ?it/s]\n",
      "inner:   0%|                                                             | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "inner:  12%|██████▋                                              | 1/8 [00:24<02:52, 24.63s/it]\u001b[A\n",
      "inner:  25%|█████████████▎                                       | 2/8 [01:08<03:35, 35.88s/it]\u001b[A\n",
      "inner:  38%|███████████████████▉                                 | 3/8 [01:42<02:54, 34.93s/it]\u001b[A\n",
      "inner:  50%|██████████████████████████▌                          | 4/8 [02:10<02:09, 32.46s/it]\u001b[A\n",
      "inner:  62%|█████████████████████████████████▏                   | 5/8 [02:47<01:42, 34.14s/it]\u001b[A\n",
      "inner:  75%|███████████████████████████████████████▊             | 6/8 [03:25<01:10, 35.25s/it]\u001b[A\n",
      "inner:  88%|██████████████████████████████████████████████▍      | 7/8 [03:58<00:34, 34.45s/it]\u001b[A\n",
      "inner: 100%|█████████████████████████████████████████████████████| 8/8 [04:26<00:00, 33.28s/it]\u001b[A\n",
      "outer:  33%|█████████████████▎                                  | 1/3 [04:26<08:52, 266.26s/it]\n",
      "inner:   0%|                                                             | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "inner:  20%|██████████▌                                          | 1/5 [00:46<03:06, 46.54s/it]\u001b[A\n",
      "inner:  40%|█████████████████████▏                               | 2/5 [01:24<02:04, 41.41s/it]\u001b[A\n",
      "inner:  60%|███████████████████████████████▊                     | 3/5 [01:51<01:09, 34.67s/it]\u001b[A\n",
      "inner:  80%|██████████████████████████████████████████▍          | 4/5 [02:23<00:33, 33.67s/it]\u001b[A\n",
      "inner: 100%|█████████████████████████████████████████████████████| 5/5 [03:01<00:00, 36.31s/it]\u001b[A\n",
      "outer:  67%|██████████████████████████████████▋                 | 2/3 [07:27<03:36, 216.45s/it]\n",
      "inner:   0%|                                                             | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "inner:  20%|██████████▌                                          | 1/5 [00:31<02:04, 31.25s/it]\u001b[A\n",
      "inner:  40%|█████████████████████▏                               | 2/5 [01:08<01:44, 34.85s/it]\u001b[A\n",
      "inner:  60%|███████████████████████████████▊                     | 3/5 [01:42<01:08, 34.45s/it]\u001b[A\n",
      "inner:  80%|██████████████████████████████████████████▍          | 4/5 [02:15<00:34, 34.02s/it]\u001b[A\n",
      "inner: 100%|█████████████████████████████████████████████████████| 5/5 [02:54<00:00, 34.94s/it]\u001b[A\n",
      "outer: 100%|████████████████████████████████████████████████████| 3/3 [10:22<00:00, 207.51s/it]\n"
     ]
    }
   ],
   "source": [
    "# Prompting and writing\n",
    "for data_type in data_dict.keys():\n",
    "    \n",
    "    # Create the directory for the data type if it does not exist\n",
    "    data_type_path = os.path.join(vault_path, data_type)\n",
    "    if not os.path.exists(data_type_path):\n",
    "        os.makedirs(data_type_path)\n",
    "        \n",
    "    for subcategory in tqdm(data_dict[data_type], desc=\"outer\", position=0):\n",
    "        \n",
    "        # Create the directory for the subcategory if it does not exist\n",
    "        subcategory_path = os.path.join(vault_path, data_type, subcategory)\n",
    "        if not os.path.exists(subcategory_path):\n",
    "                os.makedirs(subcategory_path)\n",
    "        \n",
    "        # Loop on each algorithm in given subcategory\n",
    "        for algorithm in tqdm(data_dict[data_type][subcategory], desc=\"inner\", position=1):\n",
    "            \n",
    "            #Create the GPT prompt\n",
    "            resource_prompt = f\"\"\"\n",
    "            For the {add_spaces_to_camelcase(algorithm)} model with {add_spaces_to_camelcase(data_type)} regarding {add_spaces_to_camelcase(subcategory)}, provide:\n",
    "        1. A short description of the model.\n",
    "        2. A list of the pros and cons of the model.\n",
    "        3. The three most relevant use cases.\n",
    "        4. Three great resources with relevant internet links for implementing the model.\n",
    "        5. The top 5 people with the most expertise relative to this model, with a link to their github page\n",
    "        \n",
    "        Format your response using Obsidian Flavored Markdown, and add internal links and tags when relevant\n",
    "            \"\"\"\n",
    "            # GPT API call\n",
    "            resources = generate_text(resource_prompt, GPTmodel=\"gpt-3.5-turbo\")\n",
    "            \n",
    "            # Create a .md file and path for the given model \n",
    "            file_name = f\"{algorithm}.md\"\n",
    "            file_path = os.path.join(subcategory_path, file_name)\n",
    "            \n",
    "            #Write prompt results to .md file\n",
    "            with open(file_path, \"w\") as f:\n",
    "                f.write(resources)\n",
    "                \n",
    "                # Add relevant tags with hierarchy. Strip special chars for clarity\n",
    "                f.write(f\"\\n\\n\\n ### Relevant Internal Links\\n\")\n",
    "                f.write(f\"- Data Type : [[{data_type}]]\\n\")\n",
    "                f.write(f\"- Problem type : [[{subcategory}]]\\n\")\n",
    "\n",
    "            # If debugging, uncomment line below\n",
    "            #print(resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e3ae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if contents of each categ are similar and i'm not missing categs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c7c115",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da340670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
